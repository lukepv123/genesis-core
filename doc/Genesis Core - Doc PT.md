
# ðŸ“˜ DocumentaÃ§Ã£o TÃ©cnica â€“ Genesis Core (com diagramas)

---

## ðŸ§­ SumÃ¡rio

- 1. VisÃ£o e Escopo do Projeto
        
- 2. Requisitos de Software
        
- 3. Projeto do Sistema
        
- 4. Bibliotecas e Versionamento
        

---

# 1. VisÃ£o e Escopo do Projeto

## 1.1. DescriÃ§Ã£o do problema

### 1.1.1. Contexto do projeto

O Genesis Core Ã© um assistente de voz modular e inteligente, desenvolvido para oferecer automaÃ§Ã£o e interaÃ§Ã£o por voz com mÃºltiplos agentes de IA. Ao iniciar, ele jÃ¡ utiliza o serviÃ§o de **Speech Recognition do Google** para transcrever a fala do usuÃ¡rio em texto, o que significa que **mesmo antes da ativaÃ§Ã£o do modo IA, as transcriÃ§Ãµes sÃ£o enviadas a servidores externos** para processamento. ApÃ³s o comando de ativaÃ§Ã£o ("protocol"), a entrada do usuÃ¡rio passa a ser encaminhada para o agente de IA ativo (ex.: `default_chatbot`).

### 1.1.2. Stakeholders

- **UsuÃ¡rio final:** Pessoas que desejam automatizar tarefas por voz.
    
- **Desenvolvedor:** Profissional que mantÃ©m, adapta ou expande o sistema com novos agentes e comandos.
    
- **Fornecedor de IA e SR:** Provedores de APIs para interpretaÃ§Ã£o de linguagem natural e reconhecimento de voz (ex.: OpenRouter/OpenAI, Google Speech Recognition).
    

### 1.1.3. UsuÃ¡rios

- UsuÃ¡rios domÃ©sticos interessados em automaÃ§Ã£o por voz.
    
- Profissionais tÃ©cnicos que desejam integrar o Genesis Core ao fluxo de trabalho.
    
- Desenvolvedores que desejam criar novos agentes ou comandos personalizados.
    

### 1.1.4. Premissas

- Python 3.11 ou superior.
    
- Microfone e saÃ­da de Ã¡udio funcionais.
    
- Comando de ativaÃ§Ã£o configurado em `prompts.yaml`.
    
- Chave da API externa definida na variÃ¡vel de ambiente `OPENROUTER_API_KEY`.
    
- ConexÃ£o com internet para transcriÃ§Ã£o de voz (obrigatÃ³ria).
    

---

## 1.2. VisÃ£o da soluÃ§Ã£o

### 1.2.1. DeclaraÃ§Ã£o de visÃ£o

Construir um assistente de voz flexÃ­vel, modular e extensÃ­vel, com mÃºltiplos comportamentos de IA, ativaÃ§Ã£o por voz e capacidade de executar comandos locais.

### 1.2.2. CenÃ¡rios de utilizaÃ§Ã£o do sistema

- **Modo inicial (escuta ativa com SR):** Capta voz e envia para o Google SR, interpretando comandos locais prÃ©-definidos.
    
- **AtivaÃ§Ã£o do Modo IA:** Comando "protocol" ativa o agente `default_chatbot` e envia as entradas para a IA configurada.
    
- **Troca de agente:** Comando "change to assistant mode" muda o agente para `assistant_agent`.
    
- **DesativaÃ§Ã£o da IA:** Comandos "exit" ou "deactivate" retornam ao estado inicial de escuta com SR apenas.
    

### 1.2.3. Features do produto

- Reconhecimento de fala via Google SR.
    
- MÃºltiplos agentes de IA com comportamentos distintos.
    
- Comandos e prompts configurÃ¡veis em YAML.
    
- ExecuÃ§Ã£o de comandos locais em qualquer estado.
    
- Respostas por texto (CLI) e voz (`pyttsx3`).
    

---

# 2. Requisitos de Software

## 2.1. Casos de uso

### UC01 â€“ Iniciar assistente com SR

**DescriÃ§Ã£o:** Ao iniciar, o sistema usa Google SR para transcrever voz e interpretar comandos locais.  
**Fluxo:**

1. UsuÃ¡rio inicia o sistema.
    
2. `SpeechListener` envia Ã¡udio ao Google SR.
    
3. Resposta textual Ã© processada pelo `OfflineCommandRouter`.
    

### UC02 â€“ Ativar modo IA

**DescriÃ§Ã£o:** Comando "protocol" ativa o agente padrÃ£o (`default_chatbot`).  
**Fluxo:**

1. SR reconhece "protocol".
    
2. `SessionManager` muda para modo IA.
    
3. `AgentRouter` encaminha entradas para a IA.
    

### UC03 â€“ Trocar de agente

**DescriÃ§Ã£o:** Comando "change to assistant mode" troca o comportamento ativo.  
**Fluxo:**

1. SR reconhece comando de troca.
    
2. `SessionManager` ativa `assistant_agent`.
    

### UC04 â€“ Executar comando local

**DescriÃ§Ã£o:** Comandos como "abrir navegador" sÃ£o reconhecidos e executados via `ActionExecutor` em qualquer modo.

### UC05 â€“ Desativar IA

**DescriÃ§Ã£o:** Comando "exit" ou "deactivate" retorna ao estado inicial (SR ativo, sem IA).

---

## 2.2. Requisitos funcionais

- **RF01:** Iniciar sempre com Google SR ativo.
    
- **RF02:** Executar comandos locais mesmo sem IA ativa.
    
- **RF03:** Ativar/desativar IA por comandos de voz configurÃ¡veis.
    
- **RF04:** Alternar entre agentes de IA sem reiniciar o sistema.
    
- **RF05:** Responder por voz e texto.
    

## 2.3. Requisitos nÃ£o funcionais

- **RNF01:** SR do Google exige conexÃ£o Ã  internet.
    
- **RNF02:** Troca de agentes deve ser imediata.
    
- **RNF03:** YAML centraliza configuraÃ§Ã£o de agentes e comandos.
    
- **RNF04:** Uso de CPU mÃ­nimo fora de chamadas Ã  IA.
    

---

## 2.4. ProtÃ³tipos

### 2.4.1. Interface CLI

- Terminal exibe logs e respostas textuais.
    
- Voz sintetizada acompanha as respostas.
    

### 2.4.2. Modelo de navegaÃ§Ã£o

```text
[SR Ativo]
   â†“ "protocol"
[Modo IA - default_chatbot]
   â†“ "change to assistant mode"
[Modo IA - assistant_agent]
   â†“ "deactivate"
[SR Ativo]
```

---

# 3. Projeto do Sistema

## 3.1. Estrutura

```text
src/
â”œâ”€â”€ main.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ speech_listener.py
â”‚   â”œâ”€â”€ session_manager.py
â”‚   â”œâ”€â”€ agents_router.py
â”‚   â”œâ”€â”€ offline_command_router.py
â”‚   â”œâ”€â”€ action_executor.py
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ default_chatbot.py
â”‚   â”œâ”€â”€ assistant_agent.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”œâ”€â”€ tts_engine.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts.yaml
```

## 3.2. Principais componentes

- **SpeechListener:** Captura voz e envia para Google SR.
    
- **SessionManager:** Controla agente ativo e estado da IA.
    
- **AgentRouter:** Encaminha entradas para o agente ativo.
    
- **OfflineCommandRouter:** Reconhece comandos locais.
    
- **ActionExecutor:** Executa aÃ§Ãµes no SO.
    
- **Agentes:** Definem comportamentos e prompts.
    
- **OpenRouterClient:** Envia prompts Ã  IA.
    
- **tts_engine.py:** Resposta em voz via `pyttsx3`.
    

## 3.3. Diagramas (Mermaid)

### 3.3.1. Diagrama de Classes

```mermaid
classDiagram
    direction LR

    class SpeechListener {
      +listen() str
      +microphone
      +recognizer
    }

    class SessionManager {
      -active_agent : BaseAgent
      +get_active_agent() BaseAgent
      +set_active_agent(agent: BaseAgent) void
    }

    class AgentRouter {
      -session : SessionManager
      -agents : dict[str, BaseAgent]
      +route(text: str) str
    }

    class OfflineCommandRouter {
      +agent_mode : bool
      -tts : TextToSpeechEngine
      -executor : ActionExecutor
      +route(text: str) str
      +deactivate_agent_mode() void
    }

    class ActionExecutor {
      +execute_action(code: str) str
      +open_browser() str
      +open_vscode() str
      ... // outras aÃ§Ãµes whitelisted
    }

    class TextToSpeechEngine {
      +falar(text: str) void
      -engine
    }

    class OpenRouterGPTFreeClient {
      -api_key : str
      +get_chat_response(msgs: list) str
    }

    class BaseAgent {
      <<abstract>>
      #client : OpenRouterGPTFreeClient
      #config : dict
      +handle_command(text: str) str
    }

    class DefaultChatbot {
      +handle_command(text: str) str
    }

    class AssistantAgent {
      +handle_command(text: str) str
    }

    class Main {
      +__main__
    }

    BaseAgent <|-- DefaultChatbot
    BaseAgent <|-- AssistantAgent

    AgentRouter --> SessionManager : consulta agente ativo
    AgentRouter --> BaseAgent : delega interpretaÃ§Ã£o
    OfflineCommandRouter --> TextToSpeechEngine
    OfflineCommandRouter --> ActionExecutor
    DefaultChatbot --> OpenRouterGPTFreeClient
    AssistantAgent --> OpenRouterGPTFreeClient
    Main --> SpeechListener
    Main --> SessionManager
    Main --> AgentRouter
    Main --> OfflineCommandRouter
    Main --> ActionExecutor
    Main --> TextToSpeechEngine
```

### 3.3.2. Diagrama de Fluxo (SequÃªncia: ativaÃ§Ã£o, troca de agente e execuÃ§Ã£o)

```mermaid
sequenceDiagram
    autonumber
    participant U as UsuÃ¡rio
    participant SL as SpeechListener
    participant OCR as OfflineCommandRouter
    participant SM as SessionManager
    participant AR as AgentRouter
    participant DC as DefaultChatbot
    participant AA as AssistantAgent
    participant AX as ActionExecutor

    U->>SL: fala "protocol"
    SL->>OCR: texto="protocol"
    OCR->>SM: ativa modo IA (agente default)
    Note right of SM: session.set_active_agent(default_chatbot)

    U->>SL: pergunta "Qual o clima?"
    SL->>AR: texto="Qual o clima?"
    AR->>DC: handle_command(texto)
    DC-->>AR: resposta IA
    AR-->>U: resposta por TTS/CLI

    U->>SL: fala "change to assistant mode"
    SL->>SM: set_active_agent(assistant_agent)

    U->>SL: "abrir navegador"
    SL->>AR: texto="abrir navegador"
    AR->>AA: handle_command(texto)
    AA-->>AR: "CODE: BROWSER_OPEN"
    AR->>AX: execute_action("BROWSER_OPEN")
    AX-->>U: "Navegador aberto."

    U->>SL: "deactivate"
    SL->>OCR: rota "deactivate"
    OCR->>SM: desativa agente (volta SR-only)
```

---

# 4. Bibliotecas e Versionamento

|Biblioteca|VersÃ£o recomendada|FunÃ§Ã£o|
|---|---|---|
|`SpeechRecognition`|3.10.0|Reconhecimento de fala via Google SR|
|`pyttsx3`|2.90|SÃ­ntese de voz offline|
|`PyYAML`|6.0.1|Leitura de arquivos YAML|
|`httpx`|0.27.0|Cliente HTTP para chamadas a APIs|
|`pyaudio`|0.2.13|Captura de Ã¡udio do microfone|
|`python-dotenv`|1.0.0|Carregar variÃ¡veis de ambiente|
|`os` (nativa)|-|OperaÃ§Ãµes do sistema|
|`pathlib` (nativa)|-|Caminhos multiplataforma|

> **Privacidade:** Como o Google Speech Recognition Ã© utilizado, o Ã¡udio (ou transcriÃ§Ã£o) Ã© enviado a servidores externos para conversÃ£o em texto â€” isso ocorre desde a inicializaÃ§Ã£o do sistema.

---

